# Mapping the Collaboration between Crowdsourcing and Large Language Models: A Fine-Grained Survey


# 1. Introduction
## 1.1 The current situation of crowdsourcing
* What do users really ask large language models? an initial log analysis of google bard interactions in the wild [[SIGIR 2024]](https://dl.acm.org/doi/10.1145/3626772.3657989)
* Learning from crowdsourced noisy labels: A signal processing perspective [[arXiv 2024]](https://arxiv.org/abs/2407.06902)
* Combining large language models and crowdsourcing for hybrid human-ai misinformation detection [[SIGIR 2024]](https://dl.acm.org/doi/10.1145/3626772.3657960)
* Can large language models aid in annotating speech emotional data? uncovering new frontiers [[arXiv 2023]](https://arxiv.org/abs/2307.06090)
* Towards crowd worker selection for crowdsourced testing task [[Journal of Software 2018]](http://www.jos.org.cn/jos/article/abstract/5391)
* A comparative study on annotation quality of crowdsourcing and Ilm via label aggregation [[ICASSP 2024]](https://ieeexplore.ieee.org/document/10446733)
* Cocoon: Crowdsourced testing quality maximization under context coverage constraint [[ISSRE 2017]](https://ieeexplore.ieee.org/document/8102604)
* On replacing humans with large language models in voice-based human-in-the-loop systems [[AAAI Symposium Series 2024]](https://ojs.aaai.org/index.php/AAAI-SS/article/view/28114)

## 1.2 The rise of LLMs
* From crowdsourcing to large multimodal models: Toward enhancing image data annotation with gpt-4v [[arXiv 2023]](https://arxiv.org/abs/2310.14324)
* If in a Crowdsourced Data Annotation Pipeline, a GPT-4 Can Be a Good Annotator, How Can We Help It Be an Even Better One? [[CHI 2024]](https://dl.acm.org/doi/10.1145/3613904.3642398)
* Harnessing large language models for cost-effective relevance labeling in job search systems [[AAAI Workshop 2025]](https://aaai.org/wp-content/uploads/2024/02/W22_CompJobs24_paper_185.pdf)
* Llms as workers in human-computational algorithms? replicating crowdsourcing pipelines with llms [[CHI 2025]](https://dl.acm.org/doi/10.1145/3613905.3650741)
* Redefining crowdsourced test report prioritization: An innovative approach with large language model [[Information and Software Technology 2025]](https://www.sciencedirect.com/science/article/pii/S095058492400494X)
* Efficiency and effectiveness of Ilm-based summarization of evidence in crowdsourced fact-checking [[SIGIR 2025]](https://dl.acm.org/doi/10.1145/3688658.3699071)
* Safeguarding crowdsourcing surveys from chatgpt with prompt injection [[arXiv 2023]](https://arxiv.org/abs/2306.08833)
* Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models [[arXiv 2023]](https://arxiv.org/abs/2305.14710)
* Can Ilm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls [[NeurIPS 2023]](https://proceedings.neurips.cc/paper_files/paper/2023/hash/830ae03f274a0149c4021fa39589a8c4-Abstract-Conference.html)
* Annollm: Making large language models to be better crowdsourced annotators [[arXiv 2023]](https://arxiv.org/abs/2303.16854)
* Exploring the capabilities of large language models for generating diverse design solutions [[arXiv 2024]](https://arxiv.org/abs/2405.02345)

## 1.3 The interaction trend between LLMs and crowdsourcing
* Power-up! what can generative models do for human computation workflows? [[arXiv 2023]](https://arxiv.org/abs/2307.02243)
* Training a helpful and harmless assistant with reinforcement learning from human feedback [[arXiv 2022]](https://arxiv.org/abs/2204.05862)
* Ask the experts: sourcing a high-quality nutrition counseling dataset through human-ai collaboration [[EMNLP 2024]](https://aclanthology.org/2024.findings-emnlp.762/)
* A review of public datasets in question answering research [[ACM SIGIR Forum 2021]](https://dl.acm.org/doi/10.1145/3476415.3476420)
* Chatgpt to replace crowdsourcing of paraphrases for intent classification: Higher diversity and comparable model robustness [[arXiv 2023]](https://arxiv.org/abs/2305.12947)
* Crowd-powered data mining [[arXiv 2018]](https://arxiv.org/abs/1801.03428)
* Redefining research crowdsourcing: Incorporating human feedback with Ilm-powered digital twins [[CHI 2025]](https://dl.acm.org/doi/10.1145/3613905.3650742)
* Active learning with crowd sourcing improves information retrieval [[arXiv 2023]](https://arxiv.org/abs/2309.02058)
* Towards personalized evaluation of large language models with an anonymous crowd-sourcing platform [[WWW 2024]](https://dl.acm.org/doi/10.1145/3614458.3644026)
* Gpts are multilingual annotators for sequence generation tasks [[arXiv 2024]](https://arxiv.org/abs/2402.05512)
* Crowdsourcing or ai sourcing? [[CACM 2025]](https://dl.acm.org/doi/10.1145/3680236)
* Who should be selected to perform a task in crowdsourced testing? [[COMPSAC 2017]](https://ieeexplore.ieee.org/document/8029517)
* Responsible crowdsourcing for responsible generative ai: Engaging crowds in ai auditing and evaluation [[HCOMP 2024]](https://ojs.aaai.org/index.php/HCOMP/article/view/29653)
* Congret: Benchmarking fine-grained evaluation of retrieval augmented argumentation with Ilm judges [[arXiv 2024]](https://arxiv.org/abs/2402.05206)
* Fluid transformers and creative analogies: Exploring large language models' capacity for augmenting cross-domain analogical creativity [[C&C 2023]](https://dl.acm.org/doi/10.1145/3591196.3593450)
* Is gpt-3 text indistinguishable from human text? scarecrow: A framework for scrutinizing machine text [[arXiv 2021]](https://arxiv.org/abs/2107.01294)
* Perspectives on large language models for relevance judgment [[ICTIR 2023]](https://dl.acm.org/doi/10.1145/3578337.3605128)
* Sample-efficient human evaluation of large language models via maximum discrepancy competition [[arXiv 2024]](https://arxiv.org/abs/2404.08008)
* Human evaluation of text style transfer: A user study [[ACL 2020]](https://aclanthology.org/2020.acl-main.104/)
* The use of large language models to enhance cancer clinical trial educational materials [[JNCI Cancer Spectrum 2024]](https://academic.oup.com/jncics/article/8/2/pkae019/7618991)
* Advancing content synthesis in macro-task crowd-sourcing facilitation leveraging natural language processing [[Group Decision and Negotiation 2024]](https://link.springer.com/article/10.1007/s10726-024-09879-x)
* Targen: Targeted data generation with large language models [[arXiv 2024]](https://arxiv.org/abs/2403.04022)
* From crowdsourcing to large multimodal models: Toward enhancing image data annotation with gpt-4v [[arXiv 2023]](https://arxiv.org/abs/2310.14324)
* Annollm: Making large language models to be better crowdsourced annotators [[arXiv 2023]](https://arxiv.org/abs/2303.16854)
* Wikiwhy: Answering and explaining cause-and-effect questions [[arXiv 2022]](https://arxiv.org/abs/2210.12152)
* Unnatural instructions: Tuning language models with (almost) no human labor [[arXiv 2022]](https://arxiv.org/abs/2212.09689)
* From moments to milestones: Incremental timeline summarization leveraging large language models [[ACL 2024]](https://aclanthology.org/2024.acl-long.400/)
* Learning from crowdsourced noisy labels: A signal processing perspective [[arXiv 2024]](https://arxiv.org/abs/2407.06902)
* Phalm: Building a knowledge graph from scratch by prompting humans and a language model [[arXiv 2023]](https://arxiv.org/abs/2310.07170)
* Integrating visual context into language models for situated social conversation starters [[IEEE Transactions on Affective Computing 2024]](https://ieeexplore.ieee.org/document/10450099)
* Doing personal laps: Llm-augmented dialogue construction for personalized multi-session conversational search [[SIGIR 2024]](https://dl.acm.org/doi/10.1145/3626772.3657745)
* A survey of reinforcement learning from human feedback [[arXiv 2024]](https://arxiv.org/abs/2307.14936)
* Scaling public health text annotation: Zero-shot learning vs. crowdsourcing for improved efficiency and labeling accuracy [[arXiv 2025]](https://arxiv.org/abs/2402.06150)
* Interpretable and interactive learning with human-in-the-loop [[arXiv 2016]](https://arxiv.org/abs/1606.05685)
* Can large language models aid in annotating speech emotional data? uncovering new frontiers [[arXiv 2023]](https://arxiv.org/abs/2307.06090)
* Artificial intelligence language models and the false fantasy of participatory language policies [[Working papers in Applied Linguistics and Linguistics at York 2021]](https://wpall.journals.yorku.ca/index.php/wpall/article/view/40432)
* Dialogcc: An automated pipeline for creating high-quality multi-modal dialogue dataset [[arXiv 2022]](https://arxiv.org/abs/2212.04119)
* Synthetic data generation with large language models for text classification: Potential and limitations [[arXiv 2023]](https://arxiv.org/abs/2310.07849)
* Redefining crowdsourced test report prioritization: An innovative approach with large language model [[Information and Software Technology 2025]](https://www.sciencedirect.com/science/article/pii/S095058492400494X)
* Free lunch for user experience: Crowdsourcing agents for scalable user studies [[arXiv 2025]](https://arxiv.org/abs/2405.22981)
* Robustifying safety-aligned large language models through clean data curation [[arXiv 2024]](https://arxiv.org/abs/2405.19358)
* Exploring the capabilities of large language models for generating diverse design solutions [[arXiv 2024]](https://arxiv.org/abs/2405.02345)
* Hate personified: Investigating the role of llms in content moderation [[arXiv 2024]](https://arxiv.org/abs/2310.02657)
* Help me think: A simple prompting strategy for non-experts to create customized content with models [[arXiv 2022]](https://arxiv.org/abs/2208.08232)
* Llms to replace crowdsourcing for parallel data creation? the case of text detoxification [[EMNLP 2024]](https://aclanthology.org/2023.findings-emnlp.962/)
* The coughvid crowdsourcing dataset, a corpus for the study of large-scale cough analysis algorithms [[Scientific Data 2021]](https://www.nature.com/articles/s41597-021-00937-4)
* Training language models to follow instructions with human feedback [[arXiv 2022]](https://arxiv.org/abs/2203.02155)
* Leveraging large language models (llms) to support collaborative human-ai online risk data annotation [[arXiv 2024]](https://arxiv.org/abs/2404.07926)
* A fully automated pipeline for conversational discourse annotation: Tree scheme generation and labeling with large language models [[arXiv 2025]](https://arxiv.org/abs/2404.08961)
* Perturbation augmentation for fairer nlp [[arXiv 2022]](https://arxiv.org/abs/2108.13242)
* Efficiency and effectiveness of Ilm-based summarization of evidence in crowdsourced fact-checking [[SIGIR 2025]](https://dl.acm.org/doi/10.1145/3688658.3699071)
* Wikichat: Stopping the hallucination of large language model chatbots by few-shot grounding on wikipedia [[arXiv 2023]](https://arxiv.org/abs/2305.14292)
* Hollywood in homes: Crowdsourcing data collection for activity understanding [[arXiv 2016]](https://arxiv.org/abs/1604.01753)
* Context does matter: Implications for crowdsourced evaluation labels in task-oriented dialogue systems [[arXiv 2024]](https://arxiv.org/abs/2404.09980)
* Cheap and fast-but is it good?: evaluating non-expert annotations for natural language tasks [[EMNLP 2008]](https://aclanthology.org/D08-1027/)
* Parallelparc: A scalable pipeline for generating natural-language analogies [[arXiv 2024]](https://arxiv.org/abs/2403.01139)
* Revisiting bundle recommendation for intent-aware product bundling [[ACM Transactions on Recommender Systems 2024]](https://dl.acm.org/doi/10.1145/3638423)
* Llm-driven ontology evaluation: Verifying ontology restrictions with chatgpt [[ESWC 2024]](https://link.springer.com/chapter/10.1007/978-3-031-60921-3_16)
* Near-real-time earthquake-induced fatality estimation using crowdsourced data and large-language models [[International Journal of Disaster Risk Reduction 2024]](https://www.sciencedirect.com/science/article/pii/S221242092400392X)
* Safeguarding crowdsourcing surveys from chatgpt with prompt injection [[arXiv 2023]](https://arxiv.org/abs/2306.08833)
* Characterizing crowds to better optimize worker recommendation in crowdsourced testing [[IEEE Transactions on Software Engineering 2021]](https://ieeexplore.ieee.org/document/9082855)
* Crowdsourcing intelligence for improving disaster forecasts [[The Innovation 2024]](https://www.cell.com/the-innovation/fulltext/S2666-6758(24)00118-0)
* Model-in-the-loop (milo): Accelerating multimodal ai data annotation with Ilms [[arXiv 2024]](https://arxiv.org/abs/2309.10702)
* Reframing human-ai collaboration for generating free-text explanations [[arXiv 2021]](https://arxiv.org/abs/2112.08674)
* Aligning crowd-sourced human feedback for reinforcement learning on code generation by large language models [[IEEE Transactions on Big Data 2024]](https://ieeexplore.ieee.org/document/10430671)
* Llms as workers in human-computational algorithms? replicating crowdsourcing pipelines with llms [[CHI 2025]](https://dl.acm.org/doi/10.1145/3613905.3650741)
* On the role of large language models in crowdsourcing misinformation assessment [[ICWSM 2024]](https://ojs.aaai.org/index.php/ICWSM/article/view/28906)
* Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models [[arXiv 2023]](https://arxiv.org/abs/2305.14710)
* Efficient data labeling by hierarchical crowdsourcing with large language models [[LREC-COLING 2025]](https://aclanthology.org/2024.lrec-main.1001/)
* Reasoning about procedures with natural language processing: A tutorial [[arXiv 2022]](https://arxiv.org/abs/2201.07436)
* Human-llm collaborative construction of a cantonese emotion lexicon [[arXiv 2024]](https://arxiv.org/abs/2310.11526)
* Augesc: Dialogue augmentation with large language models for emotional support conversation [[arXiv 2022]](https://arxiv.org/abs/2202.13047)
* Judging llm-as-a-judge with mt-bench and chatbot arena [[NeurIPS 2023]](https://proceedings.neurips.cc/paper_files/paper/2023/hash/334b155b9a845e2c73043d159f0f9b8c-Abstract-Conference.html)

# 2. LLMs for Crowdsourcing
## 2.0 Introduction
* Assessing educational quality: Comparative analysis of crowdsourced, expert, and ai-driven rubric applications [[HCOMP 2024]](https://ojs.aaai.org/index.php/HCOMP/article/view/29648)

## 2.1 Task generation and execution
* Perspectives on large language models for relevance judgment [[ICTIR 2023]](https://dl.acm.org/doi/10.1145/3578337.3605128)
* Sample-efficient human evaluation of large language models via maximum discrepancy competition [[arXiv 2024]](https://arxiv.org/abs/2404.08008)
* The use of large language models to enhance cancer clinical trial educational materials [[JNCI Cancer Spectrum 2024]](https://academic.oup.com/jncics/article/8/2/pkae019/7618991)
* A comparative study on annotation quality of crowdsourcing and Ilm via label aggregation [[ICASSP 2024]](https://ieeexplore.ieee.org/document/10446733)
* Exploring the capabilities of large language models for generating diverse design solutions [[arXiv 2024]](https://arxiv.org/abs/2405.02345)
* Perturbation augmentation for fairer nlp [[arXiv 2022]](https://arxiv.org/abs/2108.13242)
* Gpts are multilingual annotators for sequence generation tasks [[arXiv 2024]](https://arxiv.org/abs/2402.05512)
* Synthetic data generation with large language models for text classification: Potential and limitations [[arXiv 2023]](https://arxiv.org/abs/2310.07849)
* Reframing human-ai collaboration for generating free-text explanations [[arXiv 2021]](https://arxiv.org/abs/2112.08674)
* From moments to milestones: Incremental timeline summarization leveraging large language models [[ACL 2024]](https://aclanthology.org/2024.acl-long.400/)
* Efficiency and effectiveness of Ilm-based summarization of evidence in crowdsourced fact-checking [[SIGIR 2025]](https://dl.acm.org/doi/10.1145/3688658.3699071)
* Can large language models aid in annotating speech emotional data? uncovering new frontiers [[arXiv 2023]](https://arxiv.org/abs/2307.06090)
* Chatgpt to replace crowdsourcing of paraphrases for intent classification: Higher diversity and comparable model robustness [[arXiv 2023]](https://arxiv.org/abs/2305.12947)
* Llms to replace crowdsourcing for parallel data creation? the case of text detoxification [[EMNLP 2024]](https://aclanthology.org/2023.findings-emnlp.962/)
* Unnatural instructions: Tuning language models with (almost) no human labor [[arXiv 2022]](https://arxiv.org/abs/2212.09689)
* Near-real-time earthquake-induced fatality estimation using crowdsourced data and large-language models [[International Journal of Disaster Risk Reduction 2024]](https://www.sciencedirect.com/science/article/pii/S221242092400392X)
* Employing large language models in survey research [[Natural Language Processing Journal 2023]](https://www.sciencedirect.com/science/article/pii/S2949713X2300021X)

## 2.2 Human-machine collaborative execution paradigm
* Scaling public health text annotation: Zero-shot learning vs. crowdsourcing for improved efficiency and labeling accuracy [[arXiv 2025]](https://arxiv.org/abs/2402.06150)
* Free lunch for user experience: Crowdsourcing agents for scalable user studies [[arXiv 2025]](https://arxiv.org/abs/2405.22981)
* Power-up! what can generative models do for human computation workflows? [[arXiv 2023]](https://arxiv.org/abs/2307.02243)
* Crowdsourcing or ai sourcing? [[CACM 2025]](https://dl.acm.org/doi/10.1145/3680236)
* Human-llm collaborative construction of a cantonese emotion lexicon [[arXiv 2024]](https://arxiv.org/abs/2310.11526)
* Redefining research crowdsourcing: Incorporating human feedback with Ilm-powered digital twins [[CHI 2025]](https://dl.acm.org/doi/10.1145/3613905.3650742)
* Efficient data labeling by hierarchical crowdsourcing with large language models [[LREC-COLING 2025]](https://aclanthology.org/2024.lrec-main.1001/)
* Model-in-the-loop (milo): Accelerating multimodal ai data annotation with Ilms [[arXiv 2024]](https://arxiv.org/abs/2309.10702)

# 3. Crowdsourcing for LLMs
## 3.1 Data annotation
* Flexcrowd: Modeling the reliability of flexible annotations [[CHI 2021]](https://dl.acm.org/doi/10.1145/3411764.3445585)
* Cheap and fast-but is it good?: evaluating non-expert annotations for natural language tasks [[EMNLP 2008]](https://aclanthology.org/D08-1027/)
* Parallelparc: A scalable pipeline for generating natural-language analogies [[arXiv 2024]](https://arxiv.org/abs/2403.01139)
* Is gpt-3 text indistinguishable from human text? scarecrow: A framework for scrutinizing machine text [[arXiv 2021]](https://arxiv.org/abs/2107.01294)
* Redefining research crowdsourcing: Incorporating human feedback with Ilm-powered digital twins [[CHI 2025]](https://dl.acm.org/doi/10.1145/3613905.3650742)
* Training language models to follow instructions with human feedback [[arXiv 2022]](https://arxiv.org/abs/2203.02155)

## 3.2 Model evaluation
* Perspectives on large language models for relevance judgment [[ICTIR 2023]](https://dl.acm.org/doi/10.1145/3578337.3605128)
* Wikiwhy: Answering and explaining cause-and-effect questions [[arXiv 2022]](https://arxiv.org/abs/2210.12152)
* Generating usage-related questions for preference elicitation in conversational recommender systems [[ACM Trans. Recomm. Syst. 2024]](https://dl.acm.org/doi/10.1145/3624345)
* Human evaluation of text style transfer: A user study [[ACL 2020]](https://aclanthology.org/2020.acl-main.104/)
* Bleu: a method for automatic evaluation of machine translation [[ACL 2002]](https://aclanthology.org/P02-1040/)
* cost-effective fine-tuning of pre-trained language models with proximal policy optimization [[arXiv 2024]](https://arxiv.org/abs/2302.13284)
* The gem benchmark: Natural language generation, its evaluation and metrics [[ACL 2021]](https://aclanthology.org/2021.acl-long.348/)

## 3.3 Human preference alignment
* Llm-crowdsourced: A benchmark-free paradigm for mutual evaluation of large language models [[arXiv 2023]](https://arxiv.org/abs/2311.13023)
* Pandalm: An automatic evaluation benchmark for Ilm instruction tuning optimization [[arXiv 2023]](https://arxiv.org/abs/2306.05087)
* Aligning crowd-sourced human feedback for reinforcement learning on code generation by large language models [[IEEE Transactions on Big Data 2024]](https://ieeexplore.ieee.org/document/10430671)
* A survey of reinforcement learning from human feedback [[arXiv 2024]](https://arxiv.org/abs/2307.14936)
* Training a helpful and harmless assistant with reinforcement learning from human feedback [[arXiv 2022]](https://arxiv.org/abs/2204.05862)
* Model-in-the-loop (milo): Accelerating multimodal ai data annotation with Ilms [[arXiv 2024]](https://arxiv.org/abs/2309.10702)

## 3.4 Model debugging
* Fluid transformers and creative analogies: Exploring large language models' capacity for augmenting cross-domain analogical creativity [[C&C 2023]](https://dl.acm.org/doi/10.1145/3591196.3593450)
* Conversational ai threads for visualizing multidimensional datasets [[arXiv 2023]](https://arxiv.org/abs/2311.05590)
* Crowdsourcing intelligence for improving disaster forecasts [[The Innovation 2024]](https://www.cell.com/the-innovation/fulltext/S2666-6758(24)00118-0)
* Interpretable and interactive learning with human-in-the-loop [[arXiv 2016]](https://arxiv.org/abs/1606.05685)
* If in a Crowdsourced Data Annotation Pipeline, a GPT-4 Can Be a Good Annotator, How Can We Help It Be an Even Better One? [[CHI 2024]](https://dl.acm.org/doi/10.1145/3613904.3642398)
* Ask the experts: sourcing a high-quality nutrition counseling dataset through human-ai collaboration [[EMNLP 2024]](https://aclanthology.org/2024.findings-emnlp.762/)
* Principles of explanatory debugging to personalize interactive machine learning [[IUI 2015]](https://dl.acm.org/doi/10.1145/2678025.2701389)

# 4. Key Challenges
## 4.1 Ethics and Employment Challenges
* From crowdsourcing to large multimodal models: Toward enhancing image data annotation with gpt-4v [[arXiv 2023]](https://arxiv.org/abs/2310.14324)
* Redefining research crowdsourcing: Incorporating human feedback with Ilm-powered digital twins [[CHI 2025]](https://dl.acm.org/doi/10.1145/3613905.3650742)
* Characterizing large language models as rationalizers of knowledge-intensive tasks [[arXiv 2023]](https://arxiv.org/abs/2311.05085)
* Artificial intelligence language models and the false fantasy of participatory language policies [[Working papers in Applied Linguistics and Linguistics at York 2021]](https://wpall.journals.yorku.ca/index.php/wpall/article/view/40432)

## 4.2 Cultural Differences
* Responsible crowdsourcing for responsible generative ai: Engaging crowds in ai auditing and evaluation [[HCOMP 2024]](https://ojs.aaai.org/index.php/HCOMP/article/view/29653)
* Artificial intelligence language models and the false fantasy of participatory language policies [[Working papers in Applied Linguistics and Linguistics at York 2021]](https://wpall.journals.yorku.ca/index.php/wpall/article/view/40432)
* Hate personified: Investigating the role of llms in content moderation [[arXiv 2024]](https://arxiv.org/abs/2310.02657)
* Neuro-symbolic procedural planning with commonsense prompting [[arXiv 2022]](https://arxiv.org/abs/2206.02928)
* Towards personalized evaluation of large language models with an anonymous crowd-sourcing platform [[WWW 2024]](https://dl.acm.org/doi/10.1145/3614458.3644026)
* Words are all you need? language as an approximation for human similarity judgments [[arXiv 2022]](https://arxiv.org/abs/2206.04105)
* Trustworthy Ilms: a survey and guideline for evaluating large language models' alignment [[arXiv 2023]](https://arxiv.org/abs/2308.05374)

## 4.3 The bias of the evaluation
* Perturbation augmentation for fairer nlp [[arXiv 2022]](https://arxiv.org/abs/2108.13242)
* Llm-driven ontology evaluation: Verifying ontology restrictions with chatgpt [[ESWC 2024]](https://link.springer.com/chapter/10.1007/978-3-031-60921-3_16)
* Combining large language models and crowdsourcing for hybrid human-ai misinformation detection [[SIGIR 2024]](https://dl.acm.org/doi/10.1145/3626772.3657960)
* Wikichat: Stopping the hallucination of large language model chatbots by few-shot grounding on wikipedia [[arXiv 2023]](https://arxiv.org/abs/2305.14292)
* Human-llm collaborative construction of a cantonese emotion lexicon [[arXiv 2024]](https://arxiv.org/abs/2310.11526)

# 5. Emerging Trends and Future Directions
## 5.1 New application scenarios
* Cqm: Coverage-constrained quality maximization in crowdsourcing test [[ICSE-C 2017]](https://ieeexplore.ieee.org/document/8103134)
* Redefining research crowdsourcing: Incorporating human feedback with Ilm-powered digital twins [[CHI 2025]](https://dl.acm.org/doi/10.1145/3613905.3650742)
* Faircs-blockchain-based fair crowd-sensing scheme using trusted execution environment [[Sensors 2020]](https://www.mdpi.com/1424-8220/20/11/3253)
* Uni-rlhf: Universal platform and benchmark suite for reinforcement learning with diverse human feedback [[arXiv 2024]](https://arxiv.org/abs/2307.04279)
* Aligning crowd-sourced human feedback for reinforcement learning on code generation by large language models [[IEEE Transactions on Big Data 2024]](https://ieeexplore.ieee.org/document/10430671)

## 5.2 The evolution of crowdsourcing platforms
* Power-up! what can generative models do for human computation workflows? [[arXiv 2023]](https://arxiv.org/abs/2307.02243)
* Towards ai-empowered crowdsourcing [[arXiv 2023]](https://arxiv.org/abs/2212.14676)
* Advancing content synthesis in macro-task crowd-sourcing facilitation leveraging natural language processing [[Group Decision and Negotiation 2024]](https://link.springer.com/article/10.1007/s10726-024-09879-x)
* Llms as workers in human-computational algorithms? replicating crowdsourcing pipelines with llms [[CHI 2025]](https://dl.acm.org/doi/10.1145/3613905.3650741)
* Dynadiffuse: A dynamic diffusion model for continuous time constrained influence maximization [[AAAI 2015]](https://ojs.aaai.org/index.php/AAAI/article/view/9265)
* Interconnected neural linear contextual bandits with ucb exploration [[PAKDD 2022]](https://link.springer.com/chapter/10.1007/978-3-031-05981-0_33)
* A practical semi-parametric contextual bandit [[IJCAI 2019]](https://www.ijcai.org/proceedings/2019/379)
* Selecting workers like expert for crowdsourcing by integration evaluation of individual and collaborative abilities [[Expert Systems with Applications 2024]](https://www.sciencedirect.com/science/article/pii/S095741742401662X)
* Autobandit: A meta bandit online learning system [[IJCAI 2021]](https://www.ijcai.org/proceedings/2021/363)
* Minimizing entropy for crowdsourcing with combinatorial multi-armed bandit [[IEEE INFOCOM 2021]](https://ieeexplore.ieee.org/document/9534063)
* Panda: toward partial topology-based search on large networks in a single machine [[The VLDB Journal 2017]](https://link.springer.com/article/10.1007/s00778-016-0442-y)
* Panda: a system for partial topology-based search on large networks [[VLDB 2018]](http://www.vldb.org/pvldb/vol11/p1966-xie.pdf)
* Ferrari: an efficient framework for visual exploratory subgraph search in graph databases [[The VLDB Journal 2020]](https://link.springer.com/article/10.1007/s00778-020-00637-y)

## 5.3 Future research direction
* The dark side of recruitment in crowdsourcing: Ethics and transparency in micro-task marketplaces [[Comput. Supported Coop. Work 2023]](https://link.springer.com/article/10.1007/s10606-023-09477-z)
* Measuring social biases of crowd workers using counterfactual queries [[arXiv 2020]](https://arxiv.org/abs/2010.03844)
* Truth knows no language: Evaluating truthfulness beyond english [[arXiv 2024]](https://arxiv.org/abs/2401.07705)
* The mask benchmark: Disentangling honesty from accuracy in ai systems [[arXiv 2025]](https://arxiv.org/abs/2401.07705)
* Rebel: Rule-based and experience-enhanced learning with llms for initial task allocation in multi-human multi-robot teaming [[arXiv 2024]](https://arxiv.org/abs/2405.09700)
```
